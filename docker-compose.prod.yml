# =============================================================================
# AI Interview Tool - Production Docker Compose Configuration
# =============================================================================
# 使用方法:
#   本番環境起動:    docker-compose -f docker-compose.prod.yml up -d
#   スケール:        docker-compose -f docker-compose.prod.yml up -d --scale backend=3
#   ログ確認:        docker-compose -f docker-compose.prod.yml logs -f
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Backend API (FastAPI) - Production
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: .
      dockerfile: apps/backend/Dockerfile
      target: production
    image: ${REGISTRY:-ghcr.io}/ai-interviewer-backend:${VERSION:-latest}
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        order: start-first
    environment:
      # Database (use managed database in production)
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: ${REDIS_URL}
      # Application
      ENVIRONMENT: production
      SECRET_KEY: ${SECRET_KEY}
      DEBUG: "false"
      LOG_LEVEL: INFO
      JSON_LOGS: "true"
      # AI Provider
      AI_PROVIDER: ${AI_PROVIDER}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY}
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}
      # Speech Provider
      SPEECH_PROVIDER: ${SPEECH_PROVIDER}
      AZURE_SPEECH_KEY: ${AZURE_SPEECH_KEY}
      AZURE_SPEECH_REGION: ${AZURE_SPEECH_REGION}
      # Security
      CORS_ORIGINS: ${CORS_ORIGINS}
      RATE_LIMIT_ENABLED: "true"
      RATE_LIMIT_REQUESTS: "60"
      # Monitoring
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_ENDPOINT:-}
      OTEL_SERVICE_NAME: ai-interviewer-backend
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - app-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=PathPrefix(`/api`)"
      - "traefik.http.services.backend.loadbalancer.server.port=8000"
      - "traefik.http.services.backend.loadbalancer.healthcheck.path=/api/v1/health"

  # ---------------------------------------------------------------------------
  # Frontend Web (Next.js) - Production
  # ---------------------------------------------------------------------------
  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
      target: production
      args:
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL}
        NEXT_PUBLIC_WS_URL: ${NEXT_PUBLIC_WS_URL}
        NEXT_PUBLIC_ENVIRONMENT: production
    image: ${REGISTRY:-ghcr.io}/ai-interviewer-web:${VERSION:-latest}
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - app-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.web.rule=PathPrefix(`/`)"
      - "traefik.http.services.web.loadbalancer.server.port=3000"

  # ---------------------------------------------------------------------------
  # Traefik Reverse Proxy (Optional - for local production testing)
  # ---------------------------------------------------------------------------
  traefik:
    image: traefik:v3.2
    command:
      - "--api.dashboard=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--certificatesresolvers.letsencrypt.acme.tlschallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL:-admin@example.com}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik_certs:/letsencrypt
    networks:
      - app-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.dashboard.service=api@internal"
    profiles:
      - with-proxy

# =============================================================================
# Volumes
# =============================================================================
volumes:
  traefik_certs:

# =============================================================================
# Networks
# =============================================================================
networks:
  app-network:
    name: ai-interviewer-prod
    driver: bridge
